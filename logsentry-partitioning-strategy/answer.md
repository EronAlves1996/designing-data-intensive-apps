1. I'll choose the event_id, because I have a high workload of writes. UUID are already 'random', so the writes can be evenly distributed through the database. If server_id would be the key, it surely can create hot spots for write easily, because some server can emit more logs than others in adversity. The UUID already is a pseudohash, and because is a 128-bit key, the hash function can be simple. Key range here don't help too much because of UUID randomness, unless the UUID is timestamp sensible (like UUID v7), but it won't help for us, because timestamp sensible UUID's or any other key range criteria can create hot spots for us. Based on the read patterns, hash of key win too.
2. I would choose partitioning by document here. The actual workload for LogSentry is write centric, so the writes should have minimal overhead. Reads occurs much less time than writes, so, by partitioning by document, the write gonna be faster and more simple. The cons here is, to make some query, it should to send the query all nodes, searching each node local index, so the reads are slow for this use case.
3. The partitioning scheme choosen for the question 1 already handles this abnormal workload.
